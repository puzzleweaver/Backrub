{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import urlparse\n",
    "import re\n",
    "\n",
    "def crawlWebpage(url):\n",
    "    connection = urllib2.urlopen(url)\n",
    "    page = connection.read()\n",
    "    connection.close()\n",
    "    return page\n",
    "\n",
    "def formatText(text):\n",
    "    trimmed = text.replace('\\n', ' ')\n",
    "    trimmed = trimmed.replace('\\t', ' ')\n",
    "    trimmed = trimmed.replace('_', ' ')\n",
    "    trimmed = re.sub(r'[^\\w]', ' ', trimmed)\n",
    "    trimmed = \" \".join(trimmed.split())\n",
    "    return trimmed\n",
    "    \n",
    "\n",
    "def isValidText(text):\n",
    "    trimmed = text.replace('\\n', ' ')\n",
    "    trimmed.replace('\\t', ' ')\n",
    "    trimmed = \" \".join(trimmed.split())\n",
    "    if len(text) > 1:\n",
    "        return True\n",
    "    for i in '. /!@#$%^&*()<>,.`~·–':\n",
    "        if text == i:\n",
    "            return False\n",
    "    return text != ''\n",
    "\n",
    "def getXPath(text, tree):\n",
    "    real_text = tree.getpath(text.getparent())\n",
    "    if 'div' not in real_text:\n",
    "        return real_text\n",
    "    tags = real_text.split('/')\n",
    "    num_tags = len(tags)\n",
    "    currentElement = text.getparent()\n",
    "    ident = currentElement.get('id')\n",
    "    count = 0\n",
    "    while ident == None and count < num_tags:\n",
    "        currentElement = currentElement.getparent()\n",
    "        if currentElement == None:\n",
    "            break\n",
    "        ident = currentElement.get('id')\n",
    "        count += 1\n",
    "    if ident == None:\n",
    "        return real_text\n",
    "    else:\n",
    "        real_text = '//*[@id=\"%s\"]' %ident\n",
    "        for i in xrange(num_tags-count, num_tags):\n",
    "            tag = tags[i]\n",
    "            if 'tr' in tag and tag != 'strong':\n",
    "                if i > 0:\n",
    "                    if 'tbody' not in tags[i-1]:\n",
    "                        tag = 'tbody/' + tag\n",
    "                else:\n",
    "                    tag = 'tbody/' + tag\n",
    "            real_text += '/' + tag\n",
    "    return real_text\n",
    "    \n",
    "\n",
    "def parse_page_for_xpaths(page):\n",
    "    from cStringIO import StringIO\n",
    "    from lxml import etree\n",
    "    \n",
    "    #This is a touple array of (text, xpath)\n",
    "    xpaths = []\n",
    "    \n",
    "    parser = etree.HTMLParser(remove_blank_text=True)\n",
    "    tree   = etree.parse(StringIO(page), parser)\n",
    "    find_text = etree.XPath(\"//text()\")\n",
    "\n",
    "    for text in find_text(tree):\n",
    "        formatted_text = formatText(text)\n",
    "        if isValidText(formatted_text):\n",
    "            xpath = getXPath(text, tree)\n",
    "            if 'script' not in xpath:\n",
    "                xpaths.append((formatted_text, xpath))\n",
    "    return xpaths\n",
    "\n",
    "url = 'http://stackoverflow.com/questions/39420152/scraping-font-size-from-html-and-css'\n",
    "page = crawlWebpage(url)\n",
    "\n",
    "#This is an array of touple(text, xpath)\n",
    "xpaths = parse_page_for_xpaths(page)\n",
    "for i in xrange(0, 50):\n",
    "    text, xpath = xpaths[i]\n",
    "    print xpath + \" ~> \" + text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "def get_xpath_fontSize(xpath):\n",
    "    if '/h6/' in xpath:\n",
    "        return .83 * 16\n",
    "    if '/h5/' in xpath:\n",
    "        return .75 * 16\n",
    "    if '/h3/' in xpath:\n",
    "        return 1.17 * 16\n",
    "    if '/h2/' in xpath:\n",
    "        return 1.5 * 16\n",
    "    if '/h1/' in xpath:\n",
    "        return 2 * 16\n",
    "    return 16\n",
    "\n",
    "def get_xpath_data(xpaths):\n",
    "    xpath_data = []\n",
    "    for i in xpaths:\n",
    "        text, xpath = i\n",
    "        xpath_data.append((text, get_xpath_fontSize(xpath)))\n",
    "    return xpath_data\n",
    "\n",
    "\n",
    "#url = 'http://stackoverflow.com/questions/39420152/scraping-font-size-from-html-and-css'\n",
    "#page = crawlWebpage(url)\n",
    "\n",
    "#This is an array of touple(text, xpath)\n",
    "#xpaths = parse_page_for_xpaths(page)\n",
    "\n",
    "#this is an array of touple(text, fontSize)\n",
    "xpath_data = get_xpath_data(xpaths)\n",
    "for i in xrange(0, min(len(xpath_data), 50)):\n",
    "    text, fontSize = xpath_data[i]\n",
    "    print fontSize, \" ~> \" + text\n",
    "#//*[@id=\"sidebar\"]/div[6]/div/div[1]/a[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize_xpath_data(xpath_data, print_data = False):\n",
    "    maxFontSize = 0\n",
    "    minFontSize = 10000000#just a really high number...fix later\n",
    "    for i in xpath_data:\n",
    "        text, fontSize = i\n",
    "        fontSize = float(fontSize)\n",
    "        if maxFontSize < fontSize:\n",
    "            maxFontSize = fontSize\n",
    "        if minFontSize > fontSize:\n",
    "            minFontSize = fontSize\n",
    "    normalized_data = []\n",
    "    for i in xpath_data:\n",
    "        text, fontSize = i#fontSize is actually text. Convert it to a float\n",
    "        trueFontSize = float(fontSize)\n",
    "        normalizedFontSize = int(round(float(trueFontSize - minFontSize)/float(maxFontSize - minFontSize) * 6))\n",
    "        if print_data:\n",
    "            print fontSize, ' ~> %i'%normalizedFontSize\n",
    "        normalized_data.append((text, normalizedFontSize))\n",
    "    return normalized_data\n",
    "    \n",
    "#url = 'http://stackoverflow.com/questions/39420152/scraping-font-size-from-html-and-css'\n",
    "#page = crawlWebpage(url)\n",
    "\n",
    "#This is an array of touple(text, xpath)\n",
    "#xpaths = parse_page_for_xpaths(page)\n",
    "\n",
    "#this is an array of touple(text, fontSize)\n",
    "#xpath_data = retrieve_xpath_data(url, xpaths)\n",
    "\n",
    "normalized_fSize = normalize_xpath_data(xpath_data, print_data = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def isCapital(word):\n",
    "    return not word.islower()\n",
    "\n",
    "def breakTextIntoWordBundles(xpath_data, print_data = False):\n",
    "    wordCount = 0\n",
    "    words = []\n",
    "    longestWordLen = 0\n",
    "    longestWord = ''\n",
    "    maxWordCount = 2**12-1\n",
    "    for i in xpath_data:\n",
    "        text, fontSize = i\n",
    "        textWords = text.split(' ')\n",
    "        for word in textWords:\n",
    "            capital = isCapital(word)\n",
    "            word = word.lower()\n",
    "            words.append((word, capital, fontSize, min(wordCount, maxWordCount)))\n",
    "            wordCount += 1\n",
    "            if len(word) > longestWordLen:\n",
    "                longestWordLen = len(word)\n",
    "                longestWord = word\n",
    "    if print_data:\n",
    "        start = time.clock()\n",
    "        print start\n",
    "        print 'Word Count:       %i' %wordCount\n",
    "        print 'Longest Word: %s(%i)' %(longestWord, longestWordLen)\n",
    "        print '========================================='\n",
    "        for wordBundle in words:\n",
    "            word, capital, fSize, position = wordBundle\n",
    "            tempWord = word\n",
    "            spacesNeeded = longestWordLen - len(word)\n",
    "            for i in xrange(0, spacesNeeded):\n",
    "                tempWord += ' '\n",
    "            print tempWord + '\\tcap: %r\\tfSize: %i\\tpos: %i'%(capital, fSize, position) \n",
    "        stop = time.clock()\n",
    "        print stop\n",
    "        print stop-start, \"seconds\"\n",
    "    return words\n",
    "\n",
    "#url = 'http://stackoverflow.com/questions/39420152/scraping-font-size-from-html-and-css'\n",
    "#page = crawlWebpage(url)\n",
    "\n",
    "#This is an array of touple(text, xpath)\n",
    "#xpaths = parse_page_for_xpaths(page)\n",
    "\n",
    "#this is an array of touple(text, fontSize)\n",
    "#xpath_data = retrieve_xpath_data(url, xpaths)\n",
    "\n",
    "#Font sizes are now normalized with the rest of the document\n",
    "#normalized_fSize = normalize_xpath_data(xpath_data)\n",
    "\n",
    "#Array of touple(lowercase word, capital, normalizedFontSize, position)\n",
    "wordBundles = breakTextIntoWordBundles(normalized_fSize, print_data = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#put it all together!\n",
    "def parsePlainHits(url, print_data = False):\n",
    "    page = crawlWebpage(url)\n",
    "\n",
    "    #This is an array of touple(text, xpath)\n",
    "    xpaths = parse_page_for_xpaths(page)\n",
    "\n",
    "    #this is an array of touple(text, fontSize)\n",
    "    xpath_data = get_xpath_data(xpaths)\n",
    "\n",
    "    #Font sizes are now normalized with the rest of the document\n",
    "    normalized_fSize = normalize_xpath_data(xpath_data)\n",
    "\n",
    "    #Array of touple(lowercase word, capital, normalizedFontSize, position)\n",
    "    wordBundles = breakTextIntoWordBundles(normalized_fSize, print_data)\n",
    "    return wordBundles\n",
    "parsedPlainHits = parsePlainHits('https://www.crummy.com/software/BeautifulSoup/bs4/doc/', print_data = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now we multi thread it!\n",
    "from multiprocessing.dummy import Pool as ThreadPool \n",
    "\n",
    "urls = [\n",
    "  'http://www.python.org', \n",
    "  'http://www.python.org/about/',\n",
    "  'http://www.onlamp.com/pub/a/python/2003/04/17/metaclasses.html',\n",
    "  'http://www.python.org/doc/',\n",
    "  'http://www.python.org/download/',\n",
    "  'http://www.python.org/getit/',\n",
    "  'http://www.python.org/community/',\n",
    "  'https://wiki.python.org/moin/',\n",
    "  ]\n",
    "pool = ThreadPool(8)\n",
    "\n",
    "start = time.clock()\n",
    "results = pool.map(parsePlainHits, urls)\n",
    "\n",
    "#close the pool and wait for the work to finish \n",
    "pool.close() \n",
    "pool.join() \n",
    "stop = time.clock()\n",
    "print stop-start, 'seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('/Users/noahziems/PycharmProjects/BackRub/'))\n",
    "import Lexicon as lex\n",
    "import struct\n",
    "\n",
    "def encodeHit(cap, fSize, pos):\n",
    "    result = 0\n",
    "    result += int(cap) << 15\n",
    "    result += ((fSize << 12) & 0b0111000000000000)\n",
    "    if pos > 0b111111111111:\n",
    "        pos = 0b111111111111\n",
    "    result += (pos & 0b0000111111111111)\n",
    "    return struct.pack('H', result)\n",
    "\n",
    "def encodePlainHits(parsedPlainHits, print_data = False):\n",
    "    hitLists = {}\n",
    "    words = {}\n",
    "    for i in parsedPlainHits:\n",
    "        word, cap, fSize, pos = i\n",
    "        wordID = lex.getID(word)\n",
    "        words[wordID] = word\n",
    "        if wordID in hitLists:\n",
    "            hitLists[wordID] = hitLists[wordID] + encodeHit(cap, fSize, pos)\n",
    "        else:\n",
    "            hitLists[wordID] = encodeHit(cap, fSize, pos)\n",
    "    if print_data:\n",
    "        for hitListKey in hitLists.keys():\n",
    "            word = words[hitListKey]\n",
    "            length = len(hitLists[hitListKey])\n",
    "            print word + '[%i]\\t[%i hits]\\t[%i bytes]:' %(hitListKey,  length/2, length)\n",
    "            print hitLists[hitListKey] + \"\\n\"\n",
    "    return hitLists\n",
    "\n",
    "lex.load()\n",
    "encodedPlainHits = encodePlainHits(parsedPlainHits, print_data = True)\n",
    "sys.getsizeof(encodedPlainHits)\n",
    "lex.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Now we try to parse fancy hits\n",
    "from cStringIO import StringIO\n",
    "from lxml import etree\n",
    "import urlparse\n",
    "\n",
    "def parse_title_hits(page):\n",
    "    parser = etree.HTMLParser()\n",
    "    tree   = etree.parse(StringIO(page), parser)\n",
    "    find_titles = etree.XPath(\"//title\")\n",
    "    \n",
    "    for title in find_titles(tree):\n",
    "        text = formatText(title.text).lower()\n",
    "        print text\n",
    "parse_title_hits(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "page = crawlWebpage('http://cs231n.github.io/assignments2016/assignment1/')\n",
    "\n",
    "soup = BeautifulSoup(page, \"html.parser\")\n",
    "patt = re.compile(\"font-size:(\\d+)\")\n",
    "[(tag.text.strip(), patt.search(tag[\"style\"]).group(1)) for tag in soup.select(\"[style*=font-size]\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
